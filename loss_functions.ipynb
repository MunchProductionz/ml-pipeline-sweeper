{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b18b0c9",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5335e562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89caa44f",
   "metadata": {},
   "source": [
    "## How to define & extend a loss in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b0afa5",
   "metadata": {},
   "source": [
    "#### (A) Write a callable or an nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403442b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss(pred, target):\n",
    "    # pred, target are tensors with gradients enabled for pred\n",
    "    loss = (pred - target).abs().mean()\n",
    "    return loss\n",
    "\n",
    "class MyLoss(nn.Module):\n",
    "    def forward(self, pred, target):\n",
    "        return (pred - target).abs().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40e0670",
   "metadata": {},
   "source": [
    "Because every primitive operation in the body is differentiable and tracks the\n",
    "\n",
    "\n",
    "computation graph, you do not implement .backward() yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581738a2",
   "metadata": {},
   "source": [
    "#### (B) Use it exactly like a built-in loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8521f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = MyLoss()\n",
    "loss = criterion(logits, masks)      # ← returns scalar\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eddadbd",
   "metadata": {},
   "source": [
    "#### (C) Augment a built-in loss\n",
    "You can keep the original value and add any penalty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56526553",
   "metadata": {},
   "outputs": [],
   "source": [
    "bce = nn.BCEWithLogitsLoss()\n",
    "loss = bce(logits, masks)\n",
    "\n",
    "l2_penalty = 0.0\n",
    "for p in model.parameters():\n",
    "    l2_penalty += p.pow(2).sum()\n",
    "\n",
    "loss = loss + 1e-4 * l2_penalty      # combined objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a640f372",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba17bf21",
   "metadata": {},
   "source": [
    "### Segmentation (UNet)\n",
    "Four widely-used segmentation losses:\n",
    "1. Dice Loss\n",
    "2. Focal Loss\n",
    "3. Tversky Loss\n",
    "4. BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ba8f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------ #\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"Soft Dice loss for logits; works with BCEWithLogits-style output.\"\"\"\n",
    "    def __init__(self, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.sigmoid(logits)\n",
    "        num   = 2 * (probs * targets).sum(dim=(1,2,3)) + self.eps\n",
    "        den   = probs.sum(dim=(1,2,3)) + targets.sum(dim=(1,2,3)) + self.eps\n",
    "        dice  = num / den\n",
    "        return 1 - dice.mean()                      # minimise (1-dice)\n",
    "    \n",
    "    \n",
    "# ------------------------------------------------------------------ #\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    γ: focusing parameter; α: optional class-balancing weight (scalar or tensor)\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma: float = 2.0, alpha: float | None = 0.25,\n",
    "                 reduction: str = \"mean\"):\n",
    "        super().__init__()\n",
    "        self.gamma, self.alpha, self.reduction = gamma, alpha, reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.sigmoid(logits)\n",
    "        pt    = torch.where(targets == 1, probs, 1 - probs)   # p_t\n",
    "        log_pt = torch.log(pt.clamp(min=1e-6))\n",
    "        loss  = -(1 - pt) ** self.gamma * log_pt\n",
    "        if self.alpha is not None:\n",
    "            at = torch.where(targets == 1, self.alpha, 1 - self.alpha)\n",
    "            loss = at * loss\n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        return loss                                           # 'none'\n",
    "    \n",
    "    \n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "class TverskyLoss(nn.Module):\n",
    "    \"\"\"α/β balance FP vs FN. Dice = α = β = 0.5 ; IoU ≈ α = β = 1.\"\"\"\n",
    "    def __init__(self, alpha: float = 0.5, beta: float = 0.5,\n",
    "                 eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.alpha, self.beta, self.eps = alpha, beta, eps\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.sigmoid(logits)\n",
    "        tp = (probs * targets).sum(dim=(1,2,3))\n",
    "        fp = (probs * (1 - targets)).sum(dim=(1,2,3))\n",
    "        fn = ((1 - probs) * targets).sum(dim=(1,2,3))\n",
    "        tversky = (tp + self.eps) / (tp + self.alpha*fp + self.beta*fn + self.eps)\n",
    "        return 1 - tversky.mean()\n",
    "    \n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "class CustomBCEWithLogitsLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Numerically stable BCE loss that takes raw logits.\n",
    "    Args\n",
    "    ----\n",
    "    reduction: 'mean' | 'sum' | 'none'\n",
    "    \"\"\"\n",
    "    def __init__(self, reduction: str = \"mean\"):\n",
    "        super().__init__()\n",
    "        if reduction not in {\"mean\", \"sum\", \"none\"}:\n",
    "            raise ValueError(\"reduction must be 'mean', 'sum', or 'none'\")\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor):\n",
    "        # targets should be float tensor of 0/1 with the same shape as logits\n",
    "        max_val = torch.clamp_min(logits, 0.0)\n",
    "        loss = max_val - logits * targets + torch.log1p(torch.exp(-torch.abs(logits)))\n",
    "        if   self.reduction == \"mean\": return loss.mean()\n",
    "        elif self.reduction == \"sum\" : return loss.sum()\n",
    "        else:                          return loss    # 'none'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626ab1ab",
   "metadata": {},
   "source": [
    "Can be used interchangeably:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ac817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = DiceLoss()          # or FocalLoss(), TverskyLoss()\n",
    "loss = criterion(outputs, masks)\n",
    "\n",
    "# for U-Net / binary segmentation\n",
    "criterion = CustomBCEWithLogitsLoss()\n",
    "loss = criterion(pred_logits, target_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd76921",
   "metadata": {},
   "source": [
    "### ResNet\n",
    "Three widely-used classification losses:\n",
    "1. Focal Loss\n",
    "2. Label-Smoothing Cross-Entropy\n",
    "3. Multi-Class Hinge (SVM) Loss\n",
    "4. Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfca48ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------ #\n",
    "class FocalLossMC(nn.Module):\n",
    "    \"\"\"\n",
    "    Multiclass focal loss working on raw logits.\n",
    "    gamma: focusing parameter; alpha: weight per class or scalar\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma: float = 2.0,\n",
    "                 alpha: torch.Tensor | float | None = None,\n",
    "                 reduction: str = \"mean\"):\n",
    "        super().__init__()\n",
    "        if isinstance(alpha, float):\n",
    "            alpha = torch.tensor([alpha])\n",
    "        self.gamma, self.register_buffer('alpha', alpha if alpha is not None else None)\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        log_probs = F.log_softmax(logits, dim=1)              # (N,C)\n",
    "        probs = log_probs.exp()\n",
    "        idx   = targets.unsqueeze(1)                         # (N,1)\n",
    "        pt    = probs.gather(1, idx).squeeze(1)              # (N,)\n",
    "        log_pt = log_probs.gather(1, idx).squeeze(1)\n",
    "        loss = - (1 - pt) ** self.gamma * log_pt\n",
    "        if self.alpha is not None:\n",
    "            at = self.alpha.gather(0, targets)\n",
    "            loss = at * loss\n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        return loss\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \"\"\"\n",
    "    ε: smoothing factor; reduction: 'mean'|'sum'|'none'\n",
    "    \"\"\"\n",
    "    def __init__(self, eps: float = 0.1, reduction: str = \"mean\"):\n",
    "        super().__init__()\n",
    "        self.eps, self.reduction = eps, reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        n_classes = logits.size(1)\n",
    "        log_probs = F.log_softmax(logits, dim=1)\n",
    "\n",
    "        # Negative-log-likelihood of smoothed target distribution\n",
    "        loss = -log_probs.sum(dim=1) * (self.eps / n_classes)\n",
    "        nll  = -log_probs.gather(1, targets.unsqueeze(1)).squeeze(1)\n",
    "        loss = loss + (1 - self.eps) * nll\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        return loss\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "class MultiClassHingeLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the Crammer-Singer multiclass hinge:\n",
    "    L = mean( max_{j≠y}(0, 1 + s_j - s_y) )\n",
    "    \"\"\"\n",
    "    def __init__(self, reduction: str = \"mean\"):\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        N, C = logits.shape\n",
    "        # Gather correct class score s_y\n",
    "        s_y = logits.gather(1, targets.view(-1, 1))          # (N,1)\n",
    "        # Compute margin to every other class\n",
    "        margin = logits + 1.0 - s_y                          # broadcast\n",
    "        margin.scatter_(1, targets.view(-1, 1), 0.0)         # ignore y\n",
    "        loss = torch.clamp(margin, min=0.0).max(dim=1).values  # max_{j≠y}\n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        return loss\n",
    "    \n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "class CustomCrossEntropyLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Vanilla multi-class cross-entropy working on raw logits.\n",
    "    Args\n",
    "    ----\n",
    "    reduction: 'mean' | 'sum' | 'none'\n",
    "    \"\"\"\n",
    "    def __init__(self, reduction: str = \"mean\"):\n",
    "        super().__init__()\n",
    "        if reduction not in {\"mean\", \"sum\", \"none\"}:\n",
    "            raise ValueError(\"reduction must be 'mean', 'sum', or 'none'\")\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor):\n",
    "        # logits: (N, C, ...)   targets: (N, ...) of int64 class indices\n",
    "        log_probs = F.log_softmax(logits, dim=1)          # same dim as PyTorch CE\n",
    "        # gather the log-probability of the correct class for every sample\n",
    "        loss = -log_probs.gather(1, targets.unsqueeze(1)).squeeze(1)\n",
    "        if self.reduction == \"mean\": return loss.mean()\n",
    "        elif self.reduction == \"sum\": return loss.sum()\n",
    "        else:                          return loss        # 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8118f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = LabelSmoothingCrossEntropy(eps=0.05)\n",
    "loss = criterion(logits, labels)\n",
    "loss.backward()\n",
    "\n",
    "# for ResNet / multi-class classification\n",
    "criterion = CustomCrossEntropyLoss()\n",
    "loss = criterion(pred_logits, target_labels)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
