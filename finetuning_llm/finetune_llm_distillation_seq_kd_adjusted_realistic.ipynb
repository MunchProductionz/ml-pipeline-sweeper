{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adbec6ed",
   "metadata": {},
   "source": [
    "# FineTune LLM - SeqKD Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256b2e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, tempfile, shutil, torch, pyarrow as pa, pyarrow.ipc as pa_ipc, torch.nn.functional as F\n",
    "from datasets import Dataset, IterableDataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b627950a",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "- Adds batched teacher-generation and disk caching so RAM/VRAM never explodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36116c1a",
   "metadata": {},
   "source": [
    "**How it works differently from `finetune_llm_distillation_seq_kd.ipynb` & things to watch out for**\n",
    "- When the datasets already contain a text column (prompt + answer joined) the class skips teacher inference and trains immediately.\n",
    "- If the datasets contain only a prompt column the teacher is invoked once (batched) to create answers in memory; for very large corpora this may still spike GPU time—see the next class for streaming.\n",
    "- Loss is the standard causal-LM cross-entropy; there is no KL term, temperature, or compute_loss override."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b02c801",
   "metadata": {},
   "source": [
    "**How it works**\n",
    "- For corpora that do not fit in RAM and would time out if the teacher had to generate everything before training, use the streaming variant below. It:\n",
    "  - iterates over the prompt dataset lazily (streaming mode),\n",
    "  - generates teacher answers in mini-batches,\n",
    "  - writes each (prompt+answer) row to a temporary Arrow file,\n",
    "  - reads that file back as a memory-mapped dataset for the student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d9fa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sequence-Level Knowledge Distillation — *streaming* edition\n",
    "────────────────────────────────────────────────────────────\n",
    "• Consumes a **streaming** Hugging-Face dataset containing a `\"prompt\"` column.\n",
    "• Generates teacher answers **on-the-fly in mini-batches**,\n",
    "  writes the (prompt+answer) pairs to a temporary Arrow file,\n",
    "  then memory-maps that file for the student fine-tune.\n",
    "• Interface mirrors DistillationFineTuner, except it needs a streaming dataset.\n",
    "\n",
    "Example usage\n",
    "──────────────\n",
    "from datasets import load_dataset\n",
    "prompts = load_dataset(\"my/billion_prompt_corpus\", streaming=True, split=\"train\")\n",
    "\n",
    "stream_ft = SeqKDStreamingFineTuner(\n",
    "    teacher_model = teacher,\n",
    "    student_model = student,\n",
    "    tokenizer     = tok,\n",
    "    prompt_stream = prompts,\n",
    "    output_dir    = \"./seqkd_stream_out\",\n",
    "    num_train_epochs = 1,\n",
    "    per_device_train_batch_size = 8,\n",
    "    learning_rate = 3e-4,\n",
    "    logging_steps = 50,\n",
    "    optim = \"paged_adamw_8bit\",\n",
    ")\n",
    "stream_ft.train()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0178f1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────\n",
    "# Helper: batch-generate teacher answers\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "@torch.no_grad()\n",
    "def teacher_batch_generate(\n",
    "    teacher,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    prompts: list[str],\n",
    "    max_new_tokens: int = 128,\n",
    "):\n",
    "    \"\"\"Returns list[str] of prompt+answer.\"\"\"\n",
    "    device = next(teacher.parameters()).device\n",
    "    inputs = tokenizer(\n",
    "        prompts, return_tensors=\"pt\", padding=True, truncation=True\n",
    "    ).to(device)\n",
    "    outs = teacher.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    answers = tokenizer.batch_decode(outs, skip_special_tokens=True)\n",
    "    return [p + a for p, a in zip(prompts, answers)]\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# Main class\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "class SeqKDStreamingFineTuner(Trainer):\n",
    "    \"\"\"\n",
    "    Sequence-level KD for very-large prompt sets (streaming, disk-backed).\n",
    "    Constructor intentionally follows DistillationFineTuner signature\n",
    "    (temperature/alpha removed, prompt_stream added).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        teacher_model,\n",
    "        student_model,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        prompt_stream: IterableDataset,      # MUST be streaming dataset w/ 'prompt'\n",
    "        eval_prompts: IterableDataset | None = None,\n",
    "        *,\n",
    "        cache_dir: str | None = None,\n",
    "        batch_size: int = 32,\n",
    "        max_seq_len: int = 256,\n",
    "        max_new_tokens: int = 128,\n",
    "        prompt_field: str = \"prompt\",\n",
    "        **training_args_kwargs,\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self._tmp_dir = cache_dir or tempfile.mkdtemp(prefix=\"seqkd_cache_\")\n",
    "        arrow_path = os.path.join(self._tmp_dir, \"train.arrow\")\n",
    "\n",
    "        # 1️⃣  Stream prompts → Arrow file\n",
    "        self._stream_to_arrow(\n",
    "            arrow_path,\n",
    "            teacher_model,\n",
    "            tokenizer,\n",
    "            prompt_stream,\n",
    "            batch_size,\n",
    "            max_new_tokens,\n",
    "            prompt_field,\n",
    "        )\n",
    "\n",
    "        # Build evaluation synthetic set if provided\n",
    "        if eval_prompts is not None:\n",
    "            eval_arrow = os.path.join(self._tmp_dir, \"eval.arrow\")\n",
    "            self._stream_to_arrow(\n",
    "                eval_arrow,\n",
    "                teacher_model,\n",
    "                tokenizer,\n",
    "                eval_prompts,\n",
    "                batch_size,\n",
    "                max_new_tokens,\n",
    "                prompt_field,\n",
    "            )\n",
    "            ds_eval = Dataset.from_file(eval_arrow)\n",
    "        else:\n",
    "            ds_eval = None\n",
    "\n",
    "        ds_train = Dataset.from_file(arrow_path)\n",
    "\n",
    "        # optional random split if eval not given\n",
    "        if ds_eval is None:\n",
    "            ds_train, ds_eval = ds_train.train_test_split(\n",
    "                test_size=0.1, seed=42\n",
    "            ).values()\n",
    "\n",
    "        # 2️⃣  Tokenise (lazy map keeps memory modest)\n",
    "        def tok(ex):\n",
    "            return tokenizer(\n",
    "                ex[\"text\"],\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=max_seq_len,\n",
    "            )\n",
    "\n",
    "        ds_train_tok = ds_train.map(tok, remove_columns=[\"text\"], batched=False)\n",
    "        ds_eval_tok  = ds_eval.map(tok,  remove_columns=[\"text\"], batched=False)\n",
    "\n",
    "        # 3️⃣  Init Hugging-Face Trainer\n",
    "        data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "        args = TrainingArguments(**training_args_kwargs)\n",
    "\n",
    "        super().__init__(\n",
    "            model=student_model,\n",
    "            args=args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=ds_train_tok,\n",
    "            eval_dataset=ds_eval_tok,\n",
    "        )\n",
    "\n",
    "    # ─── utility: streaming → arrow ────────────────────────────\n",
    "    def _stream_to_arrow(\n",
    "        self,\n",
    "        arrow_path,\n",
    "        teacher,\n",
    "        tokenizer,\n",
    "        prompt_iter: IterableDataset,\n",
    "        batch_size,\n",
    "        max_new_tokens,\n",
    "        prompt_field,\n",
    "    ):\n",
    "        schema = pa.schema([(\"text\", pa.string())])\n",
    "        with pa_ipc.new_file(arrow_path, schema) as writer:\n",
    "            buffer_prompts = []\n",
    "            for sample in prompt_iter:\n",
    "                buffer_prompts.append(sample[prompt_field])\n",
    "                if len(buffer_prompts) == batch_size:\n",
    "                    texts = teacher_batch_generate(\n",
    "                        teacher, tokenizer,\n",
    "                        buffer_prompts,\n",
    "                        max_new_tokens=max_new_tokens,\n",
    "                    )\n",
    "                    writer.write_table(pa.Table.from_arrays([pa.array(texts)], schema=schema))\n",
    "                    buffer_prompts.clear()\n",
    "            # flush remainder\n",
    "            if buffer_prompts:\n",
    "                texts = teacher_batch_generate(\n",
    "                    teacher, tokenizer, buffer_prompts, max_new_tokens=max_new_tokens\n",
    "                )\n",
    "                writer.write_table(pa.Table.from_arrays([pa.array(texts)], schema=schema))\n",
    "\n",
    "    # ─── convenience helpers (same API as before) ─────────────\n",
    "    @torch.no_grad()\n",
    "    def generate(self, prompt: str, max_new_tokens: int = 40):\n",
    "        ids = self.tokenizer(prompt, return_tensors=\"pt\").to(\n",
    "            next(self.model.parameters()).device\n",
    "        )\n",
    "        out = self.model.generate(\n",
    "            **ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "        return self.tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    def push_to_hub(self, repo_id: str):\n",
    "        self.model.push_to_hub(repo_id, use_auth_token=True)\n",
    "        self.tokenizer.push_to_hub(repo_id, use_auth_token=True)\n",
    "\n",
    "    # ─── cleanup tmp dir if we created it ─────────────────────\n",
    "    def __del__(self):\n",
    "        if hasattr(self, \"_tmp_dir\") and self._tmp_dir and \"seqkd_cache_\" in self._tmp_dir:\n",
    "            shutil.rmtree(self._tmp_dir, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0f3b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ---------- Load TEACHER (frozen) ----------\n",
    "teacher = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, load_in_8bit=True, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# ---------- Build STUDENT with LoRA ----------\n",
    "student = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, load_in_8bit=True, device_map=\"auto\"\n",
    ")\n",
    "student = prepare_model_for_kbit_training(student)\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05, task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "student = get_peft_model(student, lora_cfg)\n",
    "\n",
    "# ---------- Raw prompts ----------\n",
    "PROMPTS = [\n",
    "    \"### User:\\nTranslate 'Good morning' to Spanish.\\n### Assistant:\\n\",\n",
    "    \"### User:\\nSummarise: 'The cat sat on the mat.'\\n### Assistant:\\n\",\n",
    "    \"### User:\\nList three primary colours.\\n### Assistant:\\n\",\n",
    "    \"### User:\\nWhat is 2 + 2?\\n### Assistant:\\n\",\n",
    "    \"### User:\\nRewrite 'I like apples' in the past tense.\\n### Assistant:\\n\",\n",
    "]\n",
    "\n",
    "hf_train_prompts = Dataset() # Hugging Face Dataset objects with a 'prompt' column (plain strings)\n",
    "hf_val_prompts = Dataset() # Hugging Face Dataset objects with a 'prompt' column (plain strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd3382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "device   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token        # required for padding\n",
    "\n",
    "teacher = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "student = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "student = prepare_model_for_kbit_training(student)\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "student = get_peft_model(student, lora_cfg)\n",
    "\n",
    "# Must yield dicts with a \"prompt\" string.\n",
    "train_stream = load_dataset(\n",
    "    \"my/big_prompt_corpus\",\n",
    "    split=\"train\",\n",
    "    streaming=True\n",
    ")  # IterableDataset\n",
    "\n",
    "# Must yield dicts with a \"prompt\" string.\n",
    "val_stream = load_dataset(\n",
    "    \"my/big_prompt_corpus\",\n",
    "    split=\"validation\",\n",
    "    streaming=True\n",
    ")  # IterableDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4138a140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- SeqKD fine-tuner ----------\n",
    "seqkd_stream_ft = SeqKDStreamingFineTuner(\n",
    "    teacher_model  = teacher,\n",
    "    student_model  = student,\n",
    "    tokenizer      = tokenizer,\n",
    "    prompt_stream  = train_stream,     # streaming training prompts\n",
    "    eval_prompts   = val_stream,       # optional streaming validation prompts\n",
    "    batch_size     = 64,               # teacher-generation mini-batch\n",
    "    output_dir     = \"./seqkd_stream_large\",\n",
    "    num_train_epochs = 3,\n",
    "    per_device_train_batch_size = 8,   # student update batch\n",
    "    learning_rate  = 3e-4,\n",
    "    optim          = \"paged_adamw_8bit\",\n",
    "    logging_steps  = 100,\n",
    ")\n",
    "\n",
    "\n",
    "# ---------- Train & test ----------\n",
    "seqkd_stream_ft.train()\n",
    "\n",
    "test_prompt = \"### User:\\nWhat is 2 + 2?\\n### Assistant:\\n\"\n",
    "print(\"\\n🟢  Student AFTER streaming SeqKD:\")\n",
    "print(seqkd_stream_ft.generate(test_prompt))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
