{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10ccbcc8",
   "metadata": {},
   "source": [
    "# FineTune LLM - Prefix-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5833c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu. 8-bit: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Self-contained Prefix-Tuning fine-tuning demo on TinyLlama-1.1B-Chat.\n",
    "\"\"\"\n",
    "import os\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import PrefixTuningConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "################################################################################\n",
    "# 0. Environment & constants\n",
    "################################################################################\n",
    "MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "USE_8BIT = True                 # flip to False for full-precision CPU run\n",
    "HF_PUSH_REPO = \"YOUR_USERNAME/tinyllama-lora-demo\"   # change to your org/repo\n",
    "MAX_SEQ_LEN = 256\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on {device}. 8-bit: {USE_8BIT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d9168b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrefixFineTuner:\n",
    "    \"\"\"\n",
    "    Wraps simple Prefix-Tuning fine-tuning behind an easy interface so it mirrors\n",
    "    DistillationFineTuner.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model_id: str,\n",
    "        train_dataset,\n",
    "        eval_dataset,\n",
    "        rank: int = 16,\n",
    "        alpha: int = 32,\n",
    "        dropout: float = 0.05,\n",
    "        **training_args_kwargs\n",
    "    ):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        bnb_cfg = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "        )\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_id, quantization_config=bnb_cfg, device_map=\"auto\"\n",
    "        )\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "        prefix_cfg = PrefixTuningConfig(    # Different from LoRA\n",
    "            num_virtual_tokens = 20,\n",
    "            prefix_projection = True,\n",
    "            task_type=\"CAUSAL_LM\"\n",
    "        )\n",
    "        self.model = get_peft_model(model, prefix_cfg)  # Different from LoRA\n",
    "\n",
    "        args = TrainingArguments(**training_args_kwargs)\n",
    "        self.trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=args,\n",
    "            data_collator=DataCollatorForLanguageModeling(self.tokenizer, mlm=False),\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "        )\n",
    "\n",
    "    def train(self):\n",
    "        return self.trainer.train()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, prompt: str, max_new_tokens: int = 40):\n",
    "        device = next(self.model.parameters()).device\n",
    "        ids = self.tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        out = self.model.generate(\n",
    "            **ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "        return self.tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    def push_to_hub(self, repo_id: str):\n",
    "        self.model.push_to_hub(repo_id, use_auth_token=True)\n",
    "        self.tokenizer.push_to_hub(repo_id, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f082d0a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53db334418574c3ea8e37bcb3556908c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e94c7b95ed48a0b2b9499786a748ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# --- Toy dataset --- #\n",
    "examples = [\n",
    "    {\"text\": \"### User:\\nTranslate 'Good morning' to Spanish.\\n### Assistant:\\n\"},\n",
    "    {\"text\": \"### User:\\nSummarise: 'The cat sat on the mat.'\\n### Assistant:\\n\"},\n",
    "    {\"text\": \"### User:\\nList three primary colours.\\n### Assistant:\\n\"},\n",
    "    {\"text\": \"### User:\\nWhat is 2 + 2?\\n### Assistant:\\n\"},\n",
    "    {\"text\": \"### User:\\nRewrite 'I like apples' in the past tense.\\n### Assistant:\\n\"},\n",
    "]\n",
    "def tok_fn(e): return tokenizer(\n",
    "    e[\"text\"], max_length=256, truncation=True, padding=\"max_length\"\n",
    ")\n",
    "ds = Dataset.from_list(examples).train_test_split(test_size=0.4, seed=0)\n",
    "ds_tok = ds.map(tok_fn, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a3674e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.\n",
      "None of the available devices `available_devices = None` are supported by the bitsandbytes version you have installed: `bnb_supported_devices = {'cuda', 'xpu', 'hpu', 'npu', '\"cpu\" (needs an Intel CPU and intel_extension_for_pytorch installed and compatible with the PyTorch version)', 'mps'}`. Please check the docs to see if the backend you intend to use is available and how to install it: https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "None of the available devices `available_devices = None` are supported by the bitsandbytes version you have installed: `bnb_supported_devices = {'cuda', 'xpu', 'hpu', 'npu', '\"cpu\" (needs an Intel CPU and intel_extension_for_pytorch installed and compatible with the PyTorch version)', 'mps'}`. Please check the docs to see if the backend you intend to use is available and how to install it: https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m lora_ft = \u001b[43mPrefixFineTuner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_model_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mds_tok\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mds_tok\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./prefix-tuning_out\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpaged_adamw_8bit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m lora_ft.train()\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(lora_ft.generate(\u001b[33m\"\u001b[39m\u001b[33m### User:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mWhat is 2 + 2?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m### Assistant:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mPrefixFineTuner.__init__\u001b[39m\u001b[34m(self, base_model_id, train_dataset, eval_dataset, rank, alpha, dropout, **training_args_kwargs)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenizer.pad_token = \u001b[38;5;28mself\u001b[39m.tokenizer.eos_token\n\u001b[32m     19\u001b[39m bnb_cfg = BitsAndBytesConfig(\n\u001b[32m     20\u001b[39m     load_in_8bit=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     21\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_model_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbnb_cfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     25\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m model = prepare_model_for_kbit_training(model)\n\u001b[32m     28\u001b[39m prefix_cfg = PrefixTuningConfig(    \u001b[38;5;66;03m# Different from LoRA\u001b[39;00m\n\u001b[32m     29\u001b[39m     num_virtual_tokens = \u001b[32m20\u001b[39m,\n\u001b[32m     30\u001b[39m     prefix_projection = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     31\u001b[39m     task_type=\u001b[33m\"\u001b[39m\u001b[33mCAUSAL_LM\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     32\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\henri\\Desktop\\NTNU\\5.Året\\Annet\\Prosjekter\\ml-pipeline-sweeper\\venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:571\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\henri\\Desktop\\NTNU\\5.Året\\Annet\\Prosjekter\\ml-pipeline-sweeper\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:309\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    307\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    311\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\henri\\Desktop\\NTNU\\5.Året\\Annet\\Prosjekter\\ml-pipeline-sweeper\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:4390\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4387\u001b[39m     hf_quantizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4389\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4390\u001b[39m     \u001b[43mhf_quantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4392\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4393\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4394\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4395\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4396\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4397\u001b[39m     torch_dtype = hf_quantizer.update_torch_dtype(torch_dtype)\n\u001b[32m   4398\u001b[39m     device_map = hf_quantizer.update_device_map(device_map)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\henri\\Desktop\\NTNU\\5.Året\\Annet\\Prosjekter\\ml-pipeline-sweeper\\venv\\Lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_8bit.py:81\u001b[39m, in \u001b[36mBnb8BitHfQuantizer.validate_environment\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_multi_backend_available\n\u001b[32m     80\u001b[39m bnb_multibackend_is_enabled = is_bitsandbytes_multi_backend_available()\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[43mvalidate_bnb_backend_availability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraise_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mfrom_tf\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mfrom_flax\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     85\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mConverting into 4-bit or 8-bit weights from tf/flax weights is currently not supported, please make\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     86\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m sure the weights are in PyTorch format.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     87\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\henri\\Desktop\\NTNU\\5.Året\\Annet\\Prosjekter\\ml-pipeline-sweeper\\venv\\Lib\\site-packages\\transformers\\integrations\\bitsandbytes.py:560\u001b[39m, in \u001b[36mvalidate_bnb_backend_availability\u001b[39m\u001b[34m(raise_exception)\u001b[39m\n\u001b[32m    557\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_bitsandbytes_multi_backend_available():\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_validate_bnb_multi_backend_availability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraise_exception\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _validate_bnb_cuda_backend_availability(raise_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\henri\\Desktop\\NTNU\\5.Året\\Annet\\Prosjekter\\ml-pipeline-sweeper\\venv\\Lib\\site-packages\\transformers\\integrations\\bitsandbytes.py:517\u001b[39m, in \u001b[36m_validate_bnb_multi_backend_availability\u001b[39m\u001b[34m(raise_exception)\u001b[39m\n\u001b[32m    511\u001b[39m     err_msg = (\n\u001b[32m    512\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of the available devices `available_devices = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavailable_devices\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` are supported by the bitsandbytes version you have installed: `bnb_supported_devices = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbnb_supported_devices_with_info\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    513\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease check the docs to see if the backend you intend to use is available and how to install it: https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    514\u001b[39m     )\n\u001b[32m    516\u001b[39m     logger.error(err_msg)\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(err_msg)\n\u001b[32m    519\u001b[39m logger.warning(\u001b[33m\"\u001b[39m\u001b[33mNo supported devices found for bitsandbytes multi-backend.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    520\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: None of the available devices `available_devices = None` are supported by the bitsandbytes version you have installed: `bnb_supported_devices = {'cuda', 'xpu', 'hpu', 'npu', '\"cpu\" (needs an Intel CPU and intel_extension_for_pytorch installed and compatible with the PyTorch version)', 'mps'}`. Please check the docs to see if the backend you intend to use is available and how to install it: https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend"
     ]
    }
   ],
   "source": [
    "lora_ft = PrefixFineTuner(\n",
    "    base_model_id=MODEL_ID,\n",
    "    train_dataset=ds_tok[\"train\"],\n",
    "    eval_dataset=ds_tok[\"test\"],\n",
    "    output_dir=\"./prefix-tuning_out\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=1,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=[],\n",
    ")\n",
    "lora_ft.train()\n",
    "print(lora_ft.generate(\"### User:\\nWhat is 2 + 2?\\n### Assistant:\\n\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
