{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adbec6ed",
   "metadata": {},
   "source": [
    "# FineTune LLM - SeqKD Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256b2e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, math, torch.nn.functional as F\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b627950a",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "- This approach feeds in `RAW_PROMPTS` instead of tokens, so it is not interchangable with the other notebooks.\n",
    "- See `finetune_llm_distillation_seq_kd_adjusted.ipynb` for a version that is interchangable.\n",
    "- See `finetune_llm_distillation_seq_kd_adjusted_realistic.ipynb` for a version that is more realistic in how it handles the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0178f1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────────\n",
    "# Helper: tiny wrapper to run teacher once and grab its answers\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "@torch.no_grad()\n",
    "def synthesize_with_teacher(teacher, tokenizer, prompts, max_new_tokens=64):\n",
    "    teacher.eval()\n",
    "    teacher.requires_grad_(False)\n",
    "    pairs = []\n",
    "    for p in prompts:\n",
    "        inputs = tokenizer(p, return_tensors=\"pt\").to(next(teacher.parameters()).device)\n",
    "        out = teacher.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        answer = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "        pairs.append({\"text\": p + answer})\n",
    "    return pairs\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# SeqKD Fine-tuner class\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "class SeqKDFineTuner(Trainer):\n",
    "    \"\"\"\n",
    "    Runs Sequence-Level KD:\n",
    "      1. uses the TEACHER to create (prompt, teacher_answer) pairs,\n",
    "      2. fine-tunes the STUDENT (with LoRA) on those pairs via standard CE.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        teacher_model,\n",
    "        student_model,\n",
    "        tokenizer,\n",
    "        raw_prompts,                   # list[str] or iterable\n",
    "        max_seq_len: int = 256,\n",
    "        **training_args_kwargs\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # ---------------- 1️⃣  Create synthetic dataset ---------------- #\n",
    "        print(\"🗒️  Generating teacher answers for SeqKD …\")\n",
    "        synthetic_rows = synthesize_with_teacher(\n",
    "            teacher_model, tokenizer, raw_prompts\n",
    "        )\n",
    "        dataset = Dataset.from_list(synthetic_rows).train_test_split(\n",
    "            test_size=0.2, seed=42\n",
    "        )\n",
    "\n",
    "        def tok_fn(ex):\n",
    "            return tokenizer(\n",
    "                ex[\"text\"],\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=max_seq_len,\n",
    "            )\n",
    "\n",
    "        tokenised = dataset.map(tok_fn, remove_columns=[\"text\"])\n",
    "        data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "        # -------------- 2️⃣  Standard TrainingArguments -------------- #\n",
    "        args = TrainingArguments(**training_args_kwargs)\n",
    "\n",
    "        # -------------- 3️⃣  Kick off Hugging-Face Trainer ------------- #\n",
    "        super().__init__(\n",
    "            model=student_model,\n",
    "            args=args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=tokenised[\"train\"],\n",
    "            eval_dataset=tokenised[\"test\"],\n",
    "        )\n",
    "\n",
    "    # ----- Convenience API (unchanged from previous class) ----- #\n",
    "    @torch.no_grad()\n",
    "    def generate(self, prompt: str, max_new_tokens: int = 40):\n",
    "        ids = self.tokenizer(prompt, return_tensors=\"pt\").to(\n",
    "            next(self.model.parameters()).device\n",
    "        )\n",
    "        out = self.model.generate(\n",
    "            **ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "        return self.tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    def push_to_hub(self, repo_id: str):\n",
    "        self.model.push_to_hub(repo_id, use_auth_token=True)\n",
    "        self.tokenizer.push_to_hub(repo_id, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0f3b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ---------- Load TEACHER (frozen) ----------\n",
    "teacher = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, load_in_8bit=True, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# ---------- Build STUDENT with LoRA ----------\n",
    "student = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, load_in_8bit=True, device_map=\"auto\"\n",
    ")\n",
    "student = prepare_model_for_kbit_training(student)\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05, task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "student = get_peft_model(student, lora_cfg)\n",
    "\n",
    "# ---------- Raw prompts ----------\n",
    "PROMPTS = [\n",
    "    \"### User:\\nTranslate 'Good morning' to Spanish.\\n### Assistant:\\n\",\n",
    "    \"### User:\\nSummarise: 'The cat sat on the mat.'\\n### Assistant:\\n\",\n",
    "    \"### User:\\nList three primary colours.\\n### Assistant:\\n\",\n",
    "    \"### User:\\nWhat is 2 + 2?\\n### Assistant:\\n\",\n",
    "    \"### User:\\nRewrite 'I like apples' in the past tense.\\n### Assistant:\\n\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4138a140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- SeqKD fine-tuner ----------\n",
    "seqkd_ft = SeqKDFineTuner(\n",
    "    teacher_model = teacher,\n",
    "    student_model = student,\n",
    "    tokenizer = tokenizer,\n",
    "    raw_prompts = PROMPTS,\n",
    "    output_dir = \"./seqkd_out\",\n",
    "    num_train_epochs = 1,\n",
    "    per_device_train_batch_size = 4,\n",
    "    learning_rate = 2e-4,\n",
    "    logging_steps = 1,\n",
    "    optim = \"paged_adamw_8bit\",\n",
    "    report_to = [],\n",
    ")\n",
    "\n",
    "# ---------- Train & test ----------\n",
    "seqkd_ft.train()\n",
    "test_prompt = \"### User:\\nWhat is 2 + 2?\\n### Assistant:\\n\"\n",
    "print(\"\\n🟢  Student after SeqKD:\")\n",
    "print(seqkd_ft.generate(test_prompt))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
